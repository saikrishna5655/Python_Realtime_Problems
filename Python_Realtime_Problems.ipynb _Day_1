{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlpxBIQnReUnPkjcyE5eyx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Python Task 1 – \"Customer Complaint Analyzer\"\n",
        "Scenario:\n",
        "You work at a company that receives daily customer complaints. You’re given a list of complaint texts, and your task is to analyze which keyword appears most often (to identify common issues)."
      ],
      "metadata": {
        "id": "-2tGaMX9qsw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input:\n",
        "A list of customer complaints like:"
      ],
      "metadata": {
        "id": "nTFaQ_d2q9Gz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9fvdR50zodvP"
      },
      "outputs": [],
      "source": [
        "complaints = [\n",
        "    \"App crashes after login\",\n",
        "    \"Login fails on desktop version\",\n",
        "    \"App crashes randomly during use\",\n",
        "    \"Slow loading on app\",\n",
        "    \"Login issues again\",\n",
        "    \"Crash happens after I log in\",\n",
        "    \"Loading takes forever\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cb6ea34"
      },
      "source": [
        "# Task\n",
        "processes text data to find the top 3 most frequent keywords, ignoring common English stopwords. The code should lowercase the text, remove punctuation, tokenize words, and use `collections.Counter` for frequency counting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75d18242"
      },
      "source": [
        "## Define stopwords\n",
        "\n",
        "### Subtask:\n",
        "Create a list of common English stopwords to be ignored.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55bae64c"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a list of common English stopwords as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2J0rXx5t46E",
        "outputId": "6860ea9d-aae4-4860-c933-f1dbc3a5ba47"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "921d31e8",
        "outputId": "a4cb3626-53d7-456e-f4c7-527748e83a8f"
      },
      "source": [
        "stopwords = [\n",
        "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\",\n",
        "    'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers',\n",
        "    'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
        "    'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
        "    'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
        "    'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
        "    'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
        "    'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
        "    'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',\n",
        "    'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should',\n",
        "    \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
        "    'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n",
        "    'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
        "    'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
        "]\n",
        "print(f\"Created a list of {len(stopwords)} stopwords.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created a list of 179 stopwords.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "656a9259"
      },
      "source": [
        "## Clean and tokenize\n",
        "\n",
        "### Subtask:\n",
        "Process each complaint string by converting to lowercase, removing punctuation, and splitting into individual words (tokens).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa527523"
      },
      "source": [
        "**Reasoning**:\n",
        "Process each complaint by converting to lowercase, removing punctuation, and splitting into tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "all_tokens = []\n",
        "\n",
        "for complaint in complaints:\n",
        "  # convert to lower case\n",
        "  lower_complaint = complaint.lower()\n",
        "  #remove punctualtion\n",
        "  cleaned_complaint = '' .join(char for char in lower_complaint if char not in string.punctuation)\n",
        "  # split into tokens\n",
        "  tokens = cleaned_complaint.split()\n",
        "  #Extend the all_token list\n",
        "  all_tokens.extend(tokens)\n",
        "print(all_tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOJcZy5nriUF",
        "outputId": "f730d0b3-cda3-4203-ec01-d1d90f28c707"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['app', 'crashes', 'after', 'login', 'login', 'fails', 'on', 'desktop', 'version', 'app', 'crashes', 'randomly', 'during', 'use', 'slow', 'loading', 'on', 'app', 'login', 'issues']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2b1a5b0"
      },
      "source": [
        "## Filter stopwords\n",
        "\n",
        "### Subtask:\n",
        "Remove the defined stopwords from the list of tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27b85007"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the list of tokens and remove stopwords.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7680f4e",
        "outputId": "9509047d-de03-44ad-8a43-e2fc2e680bcb"
      },
      "source": [
        "filtered_tokens = [token for token in all_tokens if token not in stopwords]\n",
        "print(filtered_tokens[:20])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['app', 'crashes', 'login', 'login', 'fails', 'desktop', 'version', 'app', 'crashes', 'randomly', 'use', 'slow', 'loading', 'app', 'login', 'issues', 'crash', 'happens', 'log', 'loading']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e07fa27"
      },
      "source": [
        "## Count word frequency\n",
        "\n",
        "### Subtask:\n",
        "Use `collections.Counter` to count the occurrences of each word.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaf8c44f"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the Counter class, create a Counter object from the filtered tokens, and print the word counts to complete the subtask of counting word occurrences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43045b89",
        "outputId": "87833c49-2239-48e7-cab8-a1ec9dbde950"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_counts = Counter(filtered_tokens)\n",
        "print(word_counts)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'app': 3, 'login': 3, 'crashes': 2, 'loading': 2, 'fails': 1, 'desktop': 1, 'version': 1, 'randomly': 1, 'use': 1, 'slow': 1, 'issues': 1, 'crash': 1, 'happens': 1, 'log': 1, 'takes': 1, 'forever': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "137c9378"
      },
      "source": [
        "## Print top keywords\n",
        "\n",
        "### Subtask:\n",
        "Identify and print the top 3 most frequent words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df2769ca"
      },
      "source": [
        "**Reasoning**:\n",
        "Use the `most_common()` method of the `word_counts` Counter object to find and print the top 3 most frequent words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1f82097",
        "outputId": "072292bc-0b95-42e5-b904-7a09e925b4af"
      },
      "source": [
        "top_3_words = word_counts.most_common(3)\n",
        "print(top_3_words)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('app', 3), ('login', 3), ('crashes', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b3b5ef0"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   A list of 179 common English stopwords was defined.\n",
        "*   The text data was successfully cleaned by converting to lowercase and removing punctuation.\n",
        "*   The cleaned text was tokenized into individual words.\n",
        "*   Stopwords were filtered out from the list of tokens.\n",
        "*   Word frequencies were counted using `collections.Counter`.\n",
        "*   The top 3 most frequent words found were 'app' (3 occurrences), 'login' (3 occurrences), and 'crashes' (2 occurrences)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------"
      ],
      "metadata": {
        "id": "1kbjlewXxKa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Python Task 2 – \"Product Sales Aggregator\"\n",
        "Scenario:\n",
        "You’re working with an e-commerce platform. Each day, they collect logs of orders. Your job is to aggregate the total quantity sold per product, and sort the products by most sold.\n",
        "\n",
        "Input:\n",
        "You get a list of transactions like this:"
      ],
      "metadata": {
        "id": "-j78XqGgwXq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders = [\n",
        "    {\"product\": \"Laptop\", \"quantity\": 2},\n",
        "    {\"product\": \"Phone\", \"quantity\": 5},\n",
        "    {\"product\": \"Laptop\", \"quantity\": 1},\n",
        "    {\"product\": \"Tablet\", \"quantity\": 3},\n",
        "    {\"product\": \"Phone\", \"quantity\": 2},\n",
        "    {\"product\": \"Tablet\", \"quantity\": 2},\n",
        "    {\"product\": \"Phone\", \"quantity\": 1}\n",
        "]\n"
      ],
      "metadata": {
        "id": "izlpF9_dwzHi"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}
